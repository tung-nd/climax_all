# Documentation:
# https://amulet-docs.azurewebsites.net/main/advanced/51_distributed.html#
# https://amulet-docs.azurewebsites.net/config_file.html

description: Training Tokenized ViT

environment:
  registry: commondockerimages.azurecr.io
  username: commondockerimages
  image: climate_pretraining:latest

code:
  local_dir: $CONFIG_DIR/../

target:
  service: aml
  name: v100x8-ded

storage:
  data:
    storage_account_name: weatherdatastorage2
    # storage_account_name: weatherml1836488272
    container_name: datasets
    mount_dir: /mnt/data
    mount_options: ["--file-cache-timeout-in-seconds=0"]

# some environment variables to ease up setting of jobs
env_defaults:
  NODES: 1
  GPUS: 8

jobs:
  - name: tokenized_vit_5.625deg_predict_all_continuous_sinusoidal_100epochs_mpi_1024dim_16head_8depth_5e-4_lr_scheduler_step_10000_beta2_0.95 # ex: simple_job_lr_05
    sku: ${NODES}x32G${GPUS} # ex: G1
    process_count_per_node: ${GPUS}
    submit_args:
      env: { AZUREML_COMPUTE_USE_COMMON_RUNTIME: True }
      container_args:
        shm_size: 650g
    command:
      - pip install -e .
      - export MKL_THREADING_LAYER=GNU
      - python src/train_vit_continuous.py --config configs/train_tokenized_vit_cmip6_continuous.yaml
        --trainer.devices=${GPUS} --trainer.strategy=ddp
        --trainer.max_epochs=100
        --data.root_dir=/mnt/data/CMIP6/MPI-ESM/5.625deg_equally_np_all_levels/
        --data.max_predict_range=28 --data.random_lead_time=True --data.hrs_each_step=6 --data.history=1 --data.interval=0
        --data.num_workers=1 --data.batch_size=16
        --model.net.decoder_depth=2 --model.net.learn_pos_emb=True --model.net.img_size=[32,64]
        --model.net.time_history=1
        --model.net.channel_agg='attention'
        --model.net.init_mode='small'
        --model.net.patch_size=2
        --model.net.depth=8
        --model.net.embed_dim=1024
        --model.net.num_heads=16
        --model.net.drop_path=0.1
        --model.net.drop_rate=0.1
        --model.lr=5e-4
        --model.weight_decay=1e-5
        --model.warmup_epochs=20000 --model.max_epochs=200000
