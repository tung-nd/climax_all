# Documentation:
# https://amulet-docs.azurewebsites.net/main/advanced/51_distributed.html#
# https://amulet-docs.azurewebsites.net/config_file.html

description: Multi GPU training for MAE

environment:
  registry: commondockerimages.azurecr.io
  username: commondockerimages
  image: climate_pretraining:latest

code:
  local_dir: $CONFIG_DIR/../

target:
  service: aml
  name: v100x4-ded

storage:
  data:
    storage_account_name: weatherdatastorage2
    # storage_account_name: weatherml1836488272
    container_name: datasets
    mount_dir: /mnt/data
    mount_options: ["--file-cache-timeout-in-seconds=0"]

# some environment variables to ease up setting of jobs
env_defaults:
  NODES: 1
  GPUS: 4

jobs:
  - name: mae_4gpus_32x4bz_1worker_1000buffer_monthly
    sku: ${NODES}x32G${GPUS} # ex: G1
    command:
      - pip install -e .
      - export MKL_THREADING_LAYER=GNU
      - export AMLT_TENSORBOARD_LOG_METRICS=train/loss,lr-AdamW,on_train_batch_start/cpu_swap_percent,on_train_batch_start/cpu_vm_percent,on_train_batch_start/cpu_percent,on_train_batch_end/cpu_swap_percent,on_train_batch_end/cpu_percent,on_train_batch_end/cpu_vm_percent
      - python src/train_mae_new.py --config configs/train_mae_new.yaml
                      --trainer.strategy=ddp --trainer.devices=${GPUS}
                      --model.reconstruct_all=True
                      --data.root_dir=/mnt/data/1.40625deg_monthly_np --data.reader=npy --data.num_workers=1 --data.batch_size=32 --data.buffer_size=1000
